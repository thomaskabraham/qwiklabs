{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "46d6c164544f"
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Training, Tuning and Deploying a PyTorch Text Classification Model on [Vertex AI](https://cloud.google.com/vertex-ai)\n",
    "## Fine-tuning pre-trained [BERT](https://huggingface.co/bert-base-cased) model for sentiment classification task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This example is inspired from Token-Classification [notebook](https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb) and [run_glue.py](https://github.com/huggingface/transformers/blob/v2.5.0/examples/run_glue.py). \n",
    "We will be fine-tuning **`bert-base-cased`** (pre-trained) model for sentiment classification task.\n",
    "You can find the details about this model at [Hugging Face Hub](https://huggingface.co/bert-base-cased).\n",
    "\n",
    "For more notebooks with the state of the art PyTorch/Tensorflow/JAX, you can explore [Hugging FaceNotebooks](https://huggingface.co/transformers/notebooks.html).\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will be using [IMDB movie review dataset](https://huggingface.co/datasets/imdb) from [Hugging Face Datasets](https://huggingface.co/datasets).\n",
    "\n",
    "### Objective\n",
    "\n",
    "How to **Build, Train, Tune and Deploy PyTorch models on [Vertex AI](https://cloud.google.com/vertex-ai)** and emphasize first class support for training and deploying PyTorch models on Vertex AI. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "This notebook covers following sections:\n",
    "\n",
    "- [Creating Notebooks instance](#Creating-Notebooks-instance-on-Google-Cloud)\n",
    "- [Training](#Training)\n",
    "    - [Run Training Locally in the Notebook](#Training-locally-in-the-notebook)\n",
    "    - [Run Training Job on Vertex AI](#Training-on-Vertex-AI)\n",
    "        - [Training with pre-built container](#Run-Custom-Job-on-Vertex-AI-Training-with-a-pre-built-container)\n",
    "        - [Training with custom container](#Run-Custom-Job-on-Vertex-AI-Training-with-custom-container)\n",
    "- [Tuning](#Hyperparameter-Tuning) \n",
    "    - [Run Hyperparameter Tuning job on Vertex AI](#Run-Hyperparameter-Tuning-Job-on-Vertex-AI)\n",
    "- [Deploying](#Deploying)\n",
    "    - [Deploying model on Vertex AI Predictions with custom container](#Deploying-model-on-Vertex AI-Predictions-with-custom-container)\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud Platform (GCP):\n",
    "\n",
    "* [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)\n",
    "* [Vertex AI Training](https://cloud.google.com/vertex-ai/docs/training/custom-training)\n",
    "* [Vertex AI Predictions](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions)\n",
    "* [Cloud Storage](https://cloud.google.com/storage)\n",
    "* [Container Registry](https://cloud.google.com/container-registry)\n",
    "* [Cloud Build](https://cloud.google.com/build) *[Optional]*\n",
    "\n",
    "Learn about [Vertex AI Pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage Pricing](https://cloud.google.com/storage/pricing) and [Cloud Build Pricing](https://cloud.google.com/build/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cda8ab8b7a04"
   },
   "source": [
    "## Creating Notebooks instance on Google Cloud\n",
    "\n",
    "This notebook assumes you are working with [PyTorch 1.9 DLVM](https://cloud.google.com/ai-platform/notebooks/docs/images) development environment with GPU runtime. You can create a Notebook instance using [Google Cloud Console](https://cloud.google.com/notebooks/docs/create-new) or [`gcloud` command](https://cloud.google.com/sdk/gcloud/reference/notebooks/instances/create).\n",
    "\n",
    "```\n",
    "gcloud notebooks instances create example-instance \\\n",
    "    --vm-image-project=deeplearning-platform-release \\\n",
    "    --vm-image-family=pytorch-1-9-cu110-notebooks \\\n",
    "    --machine-type=n1-standard-4 \\\n",
    "    --location=us-central1-a \\\n",
    "    --boot-disk-size=100 \\\n",
    "    --accelerator-core-count=1 \\\n",
    "    --accelerator-type=NVIDIA_TESLA_V100 \\\n",
    "    --install-gpu-driver \\\n",
    "    --network=default\n",
    "```\n",
    "***\n",
    "**NOTE:** You must have GPU quota before you can create instances with GPUs. Check the [quotas](https://console.cloud.google.com/iam-admin/quotas) page to ensure that you have enough GPUs available in your project. If GPUs are not listed on the quotas page or you require additional GPU quota, [request a quota increase](https://cloud.google.com/compute/quotas#requesting_additional_quota). Free Trial accounts do not receive GPU quota by default.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5178273783dd"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a1ead2cb5e3"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "1. [Install virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv) and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the command-line in a terminal shell.\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9a864c22307c"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Python dependencies required for this notebook are [Transformers](https://pypi.org/project/transformers/), [Datasets](https://pypi.org/project/datasets/) and [hypertune](https://github.com/GoogleCloudPlatform/cloudml-hypertune) will be installed in the Notebooks instance itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1fd00fa70a2a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2974254ea6be"
   },
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade transformers\n",
    "!pip -q install {USER_FLAG} --upgrade datasets\n",
    "!pip -q install {USER_FLAG} --upgrade tqdm\n",
    "!pip -q install {USER_FLAG} --upgrade cloudml-hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0c1dcadc2c8"
   },
   "source": [
    "We will be using [Vertex AI SDK for Python](https://cloud.google.com/vertex-ai/docs/start/client-libraries#python) to interact with Vertex AI services. The high-level `aiplatform` library is designed to simplify common data science workflows by using wrapper classes and opinionated defaults. \n",
    "\n",
    "#### Install Vertex AI SDK for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab3fb1551abe"
   },
   "outputs": [],
   "source": [
    "!pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f361541eff05"
   },
   "source": [
    "### Restart the Kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2d77a223d63d"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bebb3b46278"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Select a GPU runtime\n",
    "\n",
    "**Make sure you're running this notebook in a GPU runtime if you have that option. In Colab, select \"Runtime --> Change runtime type > GPU\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1e4d8f0c294"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "1. Enable following APIs in your project required for running the tutorial\n",
    "    - [Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - [Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)\n",
    "    - [Container Registry API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com)\n",
    "    - [Cloud Build API](https://console.cloud.google.com/flows/enableapi?apiid=cloudbuild.googleapis.com)\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36a4450b7c2e"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud` or `google.auth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "019e546007a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  qwiklabs-gcp-04-2329cec2130c\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-04-2329cec2130c\"  # <---CHANGE THIS TO YOUR PROJECT\n",
    "\n",
    "import os\n",
    "\n",
    "# Get your Google Cloud project ID using google.auth\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    import google.auth\n",
    "\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    print(\"Project ID: \", PROJECT_ID)\n",
    "\n",
    "# validate PROJECT_ID\n",
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    print(\n",
    "        f\"Please set your project id before proceeding to next step. Currently it's set as {PROJECT_ID}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c9906f72b18"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e90182316f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP = 20220225001735\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def get_timestamp():\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "\n",
    "TIMESTAMP = get_timestamp()\n",
    "print(f\"TIMESTAMP = {TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35094e21d888"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you submit a training job using the Cloud SDK, you upload a Python package containing your training code to a Cloud Storage bucket. Vertex AI runs the code from this package. In this tutorial, Vertex AI also saves the trained model that results from your job in the same bucket. Using this model artifact, you can then create Vertex AI model and endpoint resources in order to serve online predictions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e07102312039"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://qwiklabs-gcp-04-2329cec2130c\"  # <---CHANGE THIS TO YOUR BUCKET\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "13b11a8299d6"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = f\"gs://{PROJECT_ID}aip-{get_timestamp()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4908f26b84be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID = qwiklabs-gcp-04-2329cec2130c\n",
      "BUCKET_NAME = gs://qwiklabs-gcp-04-2329cec2130c\n",
      "REGION = us-central1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT_ID = {PROJECT_ID}\")\n",
    "print(f\"BUCKET_NAME = {BUCKET_NAME}\")\n",
    "print(f\"REGION = {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a3aae29644f"
   },
   "source": [
    "---\n",
    "\n",
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25f9882bab87"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2586e4ed72ad"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "315724257beb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "276e063f41eb"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "e894d41223e3"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import google.auth\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from google.protobuf.json_format import MessageToDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8330d87de404"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f656723ac8ed"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import ClassLabel, Sequence, load_dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          EvalPrediction, Trainer, TrainingArguments,\n",
    "                          default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "29b773f66d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: GPU\n",
      "PyTorch version : 1.9.0\n",
      "Datasets version : 1.18.3\n",
      "Transformers version : 4.16.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Datasets version : {datasets.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9805f4ffaa4d"
   },
   "outputs": [],
   "source": [
    "APP_NAME = \"finetuned-bert-classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4c7643572342"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3d8c26c6c98"
   },
   "source": [
    "# Training\n",
    "\n",
    "In this section, we will train a PyTorch model by fine-tuning pre-trained model from [Hugging Face Transformers](https://github.com/huggingface/transformers). We will train the model locally first and then on [Vertex AI training service](https://cloud.google.com/vertex-ai/docs/training/custom-training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ba110a456ac"
   },
   "source": [
    "## Training locally in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "For this example we will use [IMDB movie review dataset](https://huggingface.co/datasets/imdb) from [Hugging Face Datasets](https://huggingface.co/datasets/) for sentiment classification task. We use the [Hugging Face Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the function `load_dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset imdb (/home/jupyter/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b650bfe9e879429da837b53f4daf975f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(\"imdb\")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GWiVUF0jIrIv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of rows in training dataset 25000 and size 207.25 MB\n",
      "Total # of rows in test dataset 25000 and size 207.25 MB\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Total # of rows in training dataset {} and size {:5.2f} MB\".format(\n",
    "        datasets[\"train\"].shape[0], datasets[\"train\"].size_in_bytes / (1024 * 1024)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Total # of rows in test dataset {} and size {:5.2f} MB\".format(\n",
    "        datasets[\"test\"].shape[0], datasets[\"test\"].size_in_bytes / (1024 * 1024)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "X6HrpprwIrIz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQPMGfEFLhFd"
   },
   "source": [
    "Using the `unique` method to extract label list. This will allow us to experiment with other datasets without hard-coding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "TYw3eIHfLhFd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].unique(\"label\")\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=2):\n",
    "    assert num_examples <= len(\n",
    "        dataset\n",
    "    ), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset) - 1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset) - 1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(\n",
    "                lambda x: [typ.feature.names[i] for i in x]\n",
    "            )\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SZy5tRB_IrI7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The motion picture was, in all likelihood, made in the year 1930 and released in 1931. I would surmise that talking motion pictures had great difficulty in making the transition from the silent era. Nevertheless, this particular Zane Grey plot appears to be very weak. Also, Gary Cooper was probably just learning to act. The result is something that would not be acceptable by today's standards. For 1931, maybe. For 2004, not acceptable. Some of the actors performed well. Sadly, the Indians always get the short end in these early westerns. They were living on the land long before the white man came, but according to twisted history, they had no right to defend themselves.</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not only does the film's author, Steven Greenstreet, obviously idolize Michael Moore, but he also follows in his footsteps by using several of Moore's Propaganda film-making tactics. Moore has expertise in distracting the viewer from this focus though, while Greenstreet is obviously less skilled here.&lt;br /&gt;&lt;br /&gt;Having been privy to all of the issues surrounding Moore's speech at UVSC, I was disappointed to see that the major complaints of the community -- that Moore was being paid $40,000 of the State of Utah 's educational funds to basically promote John Kerry's campaign and to advertise his own liberal movie -- were pushed to the background by Greenstreet while lesser issues were sensationalized.&lt;br /&gt;&lt;br /&gt;The marketing methods for this video have been equally biased and objectionable... promoting the film by claiming that \"Mormon's tried to kill Moore\". Not only is this preposterous, but it defames a major religion that Greenstreet obviously has some personal issues with. I followed Moore's visit very closely, and all of the major news agencies noted that Moore's visit came and went without any credible security problems or incidents in Utah.&lt;br /&gt;&lt;br /&gt;Greenstreet has banked on this film to jump-start his film-making career to the point that he has even dropped out of film school to help accelerate this. This seems to have been a severe miscalculation though, since Moore's visits to roughly 60 other colleges and Universities across the country in 2004 diluted interest for this rather common event. Greenstreet's assumption that American audiences would be interested in this film due to the promoted religious and conservative angles doesn't seem to be well founded.&lt;br /&gt;&lt;br /&gt;Even the name of the film, This Divided State, is somewhat of a misnomer since Utah voted overwhelmingly for Bush's re-election and thus appears to be more politically unified than any other State. The division in the movie title seems more indicative of the gulf that exists in Greenstreet's ideological differences with his religion and State. If anything, I find a humorous correlation between the religious angle of this supposed documentary and Woody Allen's hilarious contention in Sleeper (1973) that, \"I was beaten up by Quakers\".</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a pre-trained Hugging Face Transformers [`Tokenizer` class](https://huggingface.co/transformers/main_classes/tokenizer.html) which tokenizes the inputs (including converting the tokens to their corresponding IDs in the pre-trained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which ensures:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pre-training this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "178b1743fd36"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "max_seq_length = 128\n",
    "model_name_or_path = \"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_fast=True,\n",
    ")\n",
    "# 'use_fast' ensure that we use fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muc7yghlLhFf"
   },
   "source": [
    "You can check type of models available with a fast tokenizer on the [big table of models](https://huggingface.co/transformers/index.html#bigtable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "a5hBlsrHIrJL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 117, 1142, 1110, 1141, 5650, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeT4rxOGLhFf"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "**NOTE:** If, as is the case here, your inputs have already been split into words, you should pass the list of words to your tokenizer with the argument `is_split_into_words=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MPunumj-LhFg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][4]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "e3-m0sGHLhFg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8667, 117, 1142, 1110, 1141, 5650, 3325, 1154, 1734, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"],\n",
    "    is_split_into_words=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G73hFICnLhFg"
   },
   "source": [
    "Note that transformers are often pre-trained with sub-word tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer. Let's look at an example of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "d8c6f3c11353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset imdb (/home/jupyter/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c7544a5e464eefb77ad46f30f7ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-77340af95956fd39.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-0d1222ec39958e2e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397c9fc28b704807940f7474f369dc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset loading repeated here to make this cell idempotent\n",
    "# Since we are over-writing datasets variable\n",
    "datasets = load_dataset(\"imdb\")\n",
    "\n",
    "# Mapping labels to ids\n",
    "# NOTE: We can extract this automatically but the `Unique` method of the datasets\n",
    "# is not reporting the label -1 which shows up in the pre-processing.\n",
    "# Hence the additional -1 term in the dictionary\n",
    "label_to_id = {1: 1, 0: 0, -1: 0}\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the input example texts\n",
    "    NOTE: The same preprocessing step(s) will be applied\n",
    "    at the time of inference as well.\n",
    "    \"\"\"\n",
    "    args = (examples[\"text\"],)\n",
    "    result = tokenizer(\n",
    "        *args, padding=\"max_length\", max_length=max_seq_length, truncation=True\n",
    "    )\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [label_to_id[example] for example in examples[\"label\"]]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# apply preprocessing function to input examples\n",
    "datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "### Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pre-trained model and fine-tune it.\n",
    "\n",
    "---\n",
    "\n",
    "***Fine Tuning*** *involves taking a model that has already been trained for a given task and then tweaking the model for another similar task. Specifically, the tweaking involves replicating all the layers in the pre-trained model including weights and parameters, except the output layer. Then adding a new output classifier layer that predicts labels for the current task. The final step is to train the output layer from scratch, while the parameters of all layers from the pre-trained model are frozen. This allows learning from the pre-trained representations and \"fine-tuning\" the higher-order feature representations more relevant for the concrete task, such as analyzing sentiments in this case.* \n",
    "\n",
    "*For the scenario in the notebook analyzing sentiments, the pre-trained BERT model already encodes a lot of information about the language as the model was trained on a large corpus of English data in a self-supervised fashion. Now we only need to slightly tune them using their outputs as features for the sentiment classification task. This means quicker development iteration on a much smaller dataset, instead of training a specific Natural Language Processing (NLP) model with a larger training dataset.*\n",
    "\n",
    "\n",
    "![BERT fine-tuning](./images/bert-model-fine-tuning.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d032e91d6c3"
   },
   "source": [
    "Since all our tasks are about token classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from the features, as seen before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "TlqNaB8jIrJW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path, num_labels=len(label_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "**NOTE:** The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pre-train the model on a masked language modeling objective and replacing it with a new head for which we don't have pre-trained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `Trainer`, we will need to define three more things. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    output_dir=\"/tmp/cls\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate, use the `batch_size` defined at the top of the notebook and customize the number of epochs for training, as well as the weight decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqynGv2DLhFk"
   },
   "source": [
    "The last thing to define for our `Trainer` is how to compute the metrics from the predictions. You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a predictions and label_ids field) and has to return a dictionary string to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "V233MNZgLhFk"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aehFUe14LhFl"
   },
   "source": [
    "Now we create the `Trainer` object and we are almost ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=datasets[\"train\"],\n",
    "    eval_dataset=datasets[\"test\"],\n",
    "    data_collator=default_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b4255e2209f"
   },
   "source": [
    "You can add callbacks to the `trainer` object to customize the behavior of the training loop such as early stopping, reporting metrics at the end of evaluation phase or taking any decisions. In the hyperparameter tuning section of this notebook, we add a callback to `trainer` for automating hyperparameter tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now fine-tune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wZHmdWPELhFl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 25000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1563\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 05:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.283969</td>\n",
       "      <td>0.879240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /tmp/cls/checkpoint-500\n",
      "Configuration saved in /tmp/cls/checkpoint-500/config.json\n",
      "Model weights saved in /tmp/cls/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/cls/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/cls/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to /tmp/cls/checkpoint-1000\n",
      "Configuration saved in /tmp/cls/checkpoint-1000/config.json\n",
      "Model weights saved in /tmp/cls/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/cls/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/cls/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /tmp/cls/checkpoint-1500\n",
      "Configuration saved in /tmp/cls/checkpoint-1500/config.json\n",
      "Model weights saved in /tmp/cls/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /tmp/cls/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/cls/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1563, training_loss=0.34753017584177753, metrics={'train_runtime': 303.3152, 'train_samples_per_second': 82.423, 'train_steps_per_second': 5.153, 'total_flos': 1644444096000000.0, 'train_loss': 0.34753017584177753, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4d3fcb41541c"
   },
   "outputs": [],
   "source": [
    "saved_model_local_path = \"./models\"\n",
    "!mkdir ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aac64fb35302"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./models\n",
      "Configuration saved in ./models/config.json\n",
      "Model weights saved in ./models/pytorch_model.bin\n",
      "tokenizer config file saved in ./models/tokenizer_config.json\n",
      "Special tokens file saved in ./models/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(saved_model_local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKASz-2vIrJi"
   },
   "source": [
    "The `evaluate` method allows you to evaluate again on the evaluation dataset or on another dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UOUcBkX8IrJi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1563' max='1563' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1563/1563 01:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "d890a62c57c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.28396886587142944,\n",
       " 'eval_accuracy': 0.8792399764060974,\n",
       " 'eval_runtime': 60.5807,\n",
       " 'eval_samples_per_second': 412.673,\n",
       " 'eval_steps_per_second': 25.8,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX97-EH5LhFm"
   },
   "source": [
    "To get the other metrics computed  such as precision, recall or F1 score for each category, we can apply the same function as before on the result of the `predict` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cae4fdf26f2"
   },
   "source": [
    "### Run predictions locally with sample examples\n",
    "\n",
    "Using the trained model, we can predict the sentiment label for an input text after applying the preprocessing function that was used during the training. We will run the predictions locally in the notebook and later show how you can deploy the model to an endpoint using [TorchServe](https://pytorch.org/serve/) on Vertex AI Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "21ab30d5518c"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"bert-base-cased\"\n",
    "label_text = {0: \"Negative\", 1: \"Positive\"}\n",
    "saved_model_path = saved_model_local_path\n",
    "\n",
    "\n",
    "def predict(input_text, saved_model_path):\n",
    "    # initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "    # preprocess and encode input text\n",
    "    tokenizer_args = (input_text,)\n",
    "    predict_input = tokenizer(\n",
    "        *tokenizer_args,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # load trained model\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(saved_model_path)\n",
    "\n",
    "    # get predictions\n",
    "    output = loaded_model(predict_input[\"input_ids\"])\n",
    "\n",
    "    # return labels\n",
    "    label_id = torch.argmax(*output.to_tuple(), dim=1)\n",
    "\n",
    "    print(f\"Review text: {input_text}\")\n",
    "    print(f\"Sentiment : {label_text[label_id.item()]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "e594d72fa8e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/jupyter/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./models/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./models.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Jaw dropping visual affects and action! One of the best I have seen to date.\n",
      "Sentiment : Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example #1\n",
    "review_text = (\n",
    "    \"\"\"Jaw dropping visual affects and action! One of the best I have seen to date.\"\"\"\n",
    ")\n",
    "predict_input = predict(review_text, saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "dd393f0d38c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /home/jupyter/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json from cache at /home/jupyter/.cache/huggingface/transformers/226a307193a9f4344264cdc76a12988448a25345ba172f2c7421f3b6810fddad.3dab63143af66769bbb35e3811f75f7e16b2320e12b7935e216bd6159ce6d9a6\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /home/jupyter/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/jupyter/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./models/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./models.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Take away the CGI and the A-list cast and you end up with film with less punch.\n",
      "Sentiment : Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example #2\n",
    "review_text = \"\"\"Take away the CGI and the A-list cast and you end up with film with less punch.\"\"\"\n",
    "predict_input = predict(review_text, saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "760ab4b88042"
   },
   "source": [
    "Validate the model artifacts written to GCS by the training code after the job completes successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c931da77e57"
   },
   "source": [
    "#### **Create a custom model handler to handle prediction requests**\n",
    "\n",
    "When predicting sentiments of the input text with the fine-tuned transformer model, it requires pre-processing of the input text and post-processing by adding name (positive/negative) to the target label (1/0) along with probability (or confidence). We create a custom handler script that is packaged with the model artifacts and TorchServe executes the code when it runs. \n",
    "\n",
    "Custom handler script does the following:\n",
    "\n",
    "- Pre-process input text before sending it to the model for inference\n",
    "- Customize how the model is invoked for inference\n",
    "- Post-process output from the model before sending back a response\n",
    "\n",
    "Please refer to the [TorchServe documentation](https://pytorch.org/serve/custom_service.html) for defining a custom handler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5ceb881740d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing models/custom_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/custom_handler.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the classification text \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.pt file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use\n",
    "        Loads labels to name mapping file for post-processing inference response\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pt or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "\n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning('Missing the index_to_name.json file. Inference output will default.')\n",
    "            self.mapping = {\"0\": \"Negative\",  \"1\": \"Positive\"}\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "        \"\"\"\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "        sentences = text.decode('utf-8')\n",
    "        logger.info(\"Received text: '%s'\", sentences)\n",
    "\n",
    "        # Tokenize the texts\n",
    "        tokenizer_args = ((sentences,))\n",
    "        inputs = self.tokenizer(*tokenizer_args,\n",
    "                                padding='max_length',\n",
    "                                max_length=128,\n",
    "                                truncation=True,\n",
    "                                return_tensors = \"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        prediction = self.model(inputs['input_ids'].to(self.device))[0].argmax().item()\n",
    "\n",
    "        if self.mapping:\n",
    "            prediction = self.mapping[str(prediction)]\n",
    "\n",
    "        logger.info(\"Model predicted: '%s'\", prediction)\n",
    "        return [prediction]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f32fe0d49feb"
   },
   "source": [
    "##### **Generate target label to name file** *[Optional]*\n",
    "\n",
    "In the custom handler, we refer to a mapping file between target labels and their meaningful names that will be used to format the prediction response. Here we are mapping target label \"0\" as \"Negative\" and \"1\"  as \"Positive\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "1be04c0f8960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/index_to_name.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./models/index_to_name.json\n",
    "\n",
    "{\n",
    "    \"0\": \"Negative\", \n",
    "    \"1\": \"Positive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3feca8d40a3"
   },
   "source": [
    "#### **Create custom container image to serve predictions**\n",
    "\n",
    "We will use Cloud Build to create the custom container image with following build steps:\n",
    "\n",
    "##### **Download model artifacts**\n",
    "\n",
    "Download model artifacts that were saved as part of the training (or hyperparameter tuning) job from Cloud Storage to local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b44d55f7be1"
   },
   "source": [
    "##### **Build the container image**\n",
    "\n",
    "Create a Dockerfile with TorchServe as base image:\n",
    "\n",
    " - **`RUN`**: Installs dependencies such as `transformers`\n",
    " - **`COPY`**: Add model artifacts to `/home/model-server/` directory of the container image\n",
    " - **`COPY`**: Add custom handler script to `/home/model-server/` directory of the container image\n",
    " - **`RUN`**: Create `/home/model-server/config.properties` to define the serving configuration (health and prediction listener ports)\n",
    " - **`RUN`**: Run [Torch model archiver](https://pytorch.org/serve/model-archiver.html) to create a model archive file from the files copied into the image `/home/model-server/`. The model archive is saved in the `/home/model-server/model-store/` with name same as `<model-name>.mar`\n",
    " - **`CMD`**: Launch Torchserve HTTP server referencing the configuration properties and enables serving for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p models/model/finetuned-bert-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp models/*.* models/model/finetuned-bert-classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "54acc94e5949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./models/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat << EOF > ./models/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY ./custom_handler.py /home/model-server/\n",
    "COPY ./index_to_name.json /home/model-server/\n",
    "COPY ./model/$APP_NAME/ /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "USER model-server\n",
    "\n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/training_args.bin,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt,/home/model-server/index_to_name.json\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--models\", \\\n",
    "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./models/Dockerfile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8878a9e37fe"
   },
   "source": [
    "Build the docker image tagged with Container Registry (gcr.io) path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "d2d6b9a4857c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = gcr.io/qwiklabs-gcp-04-2329cec2130c/pytorch_predict_finetuned-bert-classifier\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertex-ai-samples/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  868.5MB\n",
      "Step 1/16 : FROM pytorch/torchserve:latest-cpu\n",
      " ---> 659d9f4840d5\n",
      "Step 2/16 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 8cf6b2f7245c\n",
      "Step 3/16 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> d46f2af22170\n",
      "Step 4/16 : USER model-server\n",
      " ---> Using cache\n",
      " ---> cfcff0b5e91e\n",
      "Step 5/16 : COPY ./custom_handler.py /home/model-server/\n",
      " ---> Using cache\n",
      " ---> 25be97b91b5d\n",
      "Step 6/16 : COPY ./index_to_name.json /home/model-server/\n",
      " ---> Using cache\n",
      " ---> ea4d3e0af699\n",
      "Step 7/16 : COPY ./model/finetuned-bert-classifier/ /home/model-server/\n",
      " ---> 8fc062d12ecd\n",
      "Step 8/16 : USER root\n",
      " ---> Running in 9e561dbcaa77\n",
      "Removing intermediate container 9e561dbcaa77\n",
      " ---> 6f336fcf0259\n",
      "Step 9/16 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in 018379ae7975\n",
      "Removing intermediate container 018379ae7975\n",
      " ---> 8ff5d3ab6e10\n",
      "Step 10/16 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in da173cf45d58\n",
      "Removing intermediate container da173cf45d58\n",
      " ---> 87480a417f24\n",
      "Step 11/16 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 47dc444f48b9\n",
      "Removing intermediate container 47dc444f48b9\n",
      " ---> 936e98170cf3\n",
      "Step 12/16 : USER model-server\n",
      " ---> Running in 13f1f5c9255b\n",
      "Removing intermediate container 13f1f5c9255b\n",
      " ---> 14567ae3edf8\n",
      "Step 13/16 : EXPOSE 7080\n",
      " ---> Running in 265b75e78a68\n",
      "Removing intermediate container 265b75e78a68\n",
      " ---> a8e702970f3a\n",
      "Step 14/16 : EXPOSE 7081\n",
      " ---> Running in 3fe1e56d7d75\n",
      "Removing intermediate container 3fe1e56d7d75\n",
      " ---> 54fc06e71ecf\n",
      "Step 15/16 : RUN torch-model-archiver -f   --model-name=finetuned-bert-classifier   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/training_args.bin,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json,/home/model-server/vocab.txt,/home/model-server/index_to_name.json\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in 508f1849fc28\n",
      "Removing intermediate container 508f1849fc28\n",
      " ---> e97b7676ae04\n",
      "Step 16/16 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"finetuned-bert-classifier=finetuned-bert-classifier.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 6b0e8470253d\n",
      "Removing intermediate container 6b0e8470253d\n",
      " ---> 38e39e538d1b\n",
      "Successfully built 38e39e538d1b\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-2329cec2130c/pytorch_predict_finetuned-bert-classifier:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI\\\n",
    "./models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e13aeb92678"
   },
   "source": [
    "#### **Run the container locally** ***[Optional]***\n",
    "\n",
    "Before push the container image to Container Registry to use it with Vertex AI Predictions, you can run it as a container in your local environment to verify that the server works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaf5bb210eb9"
   },
   "source": [
    "1. To run the container image as a container locally, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "5b848c5fec2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_bert_classifier\n",
      "76ef383d9bcbdb638109e97d712a02c8ca20498debe6f0ce7d2d4a1eff446c81\n"
     ]
    }
   ],
   "source": [
    "!docker stop local_bert_classifier\n",
    "!docker run -t -d --rm -p 7080:7080 --name=local_bert_classifier $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a63999a79fea"
   },
   "source": [
    "2. To send the container's server a health check, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "6499006af599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a45d57cf5ba"
   },
   "source": [
    "If successful, the server returns the following response:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"status\": \"Healthy\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6413202bfeb8"
   },
   "source": [
    "3. To send the container's server a prediction request, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "246d76732fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [\"Negative\"]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./predictor/instances.json <<END\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'Take away the CGI and the A-list cast and you end up with film with less punch.' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d34a279f7fe7"
   },
   "source": [
    "This request uses a test sentence. If successful, the server returns prediction in below format:\n",
    "\n",
    "```\n",
    "    {\"predictions\": [\"Negative\"]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ac697916268"
   },
   "source": [
    "4. To stop the container, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0a116f4fa88c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_bert_classifier\n"
     ]
    }
   ],
   "source": [
    "!docker stop local_bert_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69477b3a00c0"
   },
   "source": [
    "#### **Deploying the serving container to Vertex AI Predictions**\n",
    "\n",
    "We create a model resource on Vertex AI and deploy the model to a Vertex AI Endpoints. You must deploy a model to an endpoint before using the model. The deployed model runs the custom container image to serve predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "650995567bb1"
   },
   "source": [
    "##### **Push the serving container to Container Registry**\n",
    "\n",
    "Push your container image with inference code and dependencies to your Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "56a927f34d2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-2329cec2130c/pytorch_predict_finetuned-bert-classifier]\n",
      "\n",
      "\u001b[1Be0c98c08: Preparing \n",
      "\u001b[1B3db392f7: Preparing \n",
      "\u001b[1B49d0c584: Preparing \n",
      "\u001b[1B94df0d00: Preparing \n",
      "\u001b[1Bd8b5834e: Preparing \n",
      "\u001b[1B9deb5540: Preparing \n",
      "\u001b[1Ba4adc98d: Preparing \n",
      "\u001b[1B4bd357e5: Preparing \n",
      "\u001b[1Bc44f0ee2: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1Bd2b4edc6: Preparing \n",
      "\u001b[1B4d0230ad: Preparing \n",
      "\u001b[1Bf03ffca8: Preparing \n",
      "\u001b[1Bf2874ea9: Preparing \n",
      "\u001b[1Ba1d7f3ba: Preparing \n",
      "\u001b[1Bd7c91a07: Preparing \n",
      "\u001b[1Bc3f5e5be: Preparing \n",
      "\u001b[1B512fd434: Preparing \n",
      "\u001b[1B31fc0e08: Preparing \n",
      "\u001b[20B0c98c08: Pushed   836.7MB/836.7MB\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[18A\u001b[2K\u001b[16A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[20A\u001b[2K\u001b[2A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2KPushing  171.5MB/836.7MB\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2Klatest: digest: sha256:bf3cd86a95438e80a967ac10d9285c2119d6253a36a9cecca6fd893036cda2b8 size: 4498\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3da91e19af4"
   },
   "source": [
    "##### **Initialize the Vertex AI SDK for Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "4e0857c467ef"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a8e5f9ef00b"
   },
   "source": [
    "##### **Create a Model resource with custom serving container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ad5f86202958"
   },
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based text classifier with custom container\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "c7fb34080960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/1087289348300/locations/us-central1/models/3938648562786631680/operations/1390913613937508352\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/1087289348300/locations/us-central1/models/3938648562786631680\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/1087289348300/locations/us-central1/models/3938648562786631680')\n",
      "finetuned-bert-classifier-v1\n",
      "projects/1087289348300/locations/us-central1/models/3938648562786631680\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85134f5adfa5"
   },
   "source": [
    "For more context on upload or importing a model, refer [documentation](https://cloud.google.com/vertex-ai/docs/general/import-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12774f0dc68"
   },
   "source": [
    "##### **Create an Endpoint for Model with Custom Container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "d0bf59607969"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/1087289348300/locations/us-central1/endpoints/1220800954459226112/operations/402373495729684480\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/1087289348300/locations/us-central1/endpoints/1220800954459226112\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/1087289348300/locations/us-central1/endpoints/1220800954459226112')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba9826243edf"
   },
   "source": [
    "##### **Deploy the Model to Endpoint**\n",
    "\n",
    "Deploying a model associates physical resources with the model so it can serve online predictions with low latency. \n",
    "\n",
    "**NOTE:** This step takes few minutes to deploy the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8e36ccbc24b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/1087289348300/locations/us-central1/endpoints/1220800954459226112\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/1087289348300/locations/us-central1/endpoints/1220800954459226112/operations/5014059514157072384\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/1087289348300/locations/us-central1/endpoints/1220800954459226112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f04bb2dc250> \n",
       "resource name: projects/1087289348300/locations/us-central1/endpoints/1220800954459226112"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bc4673478269"
   },
   "source": [
    "#### **Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fcbf1497407"
   },
   "source": [
    "##### **Get the Endpoint id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "52362d052767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name = finetuned-bert-classifier-endpoint resource id =projects/1087289348300/locations/us-central1/endpoints/1220800954459226112 \n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "376e74fdbd95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"3203633835711397888\"\n",
       "model: \"projects/1087289348300/locations/us-central1/models/3938648562786631680\"\n",
       "display_name: \"finetuned-bert-classifier-v1\"\n",
       "create_time {\n",
       "  seconds: 1645751038\n",
       "  nanos: 261676000\n",
       "}\n",
       "dedicated_resources {\n",
       "  machine_spec {\n",
       "    machine_type: \"n1-standard-4\"\n",
       "  }\n",
       "  min_replica_count: 1\n",
       "  max_replica_count: 1\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f454fff5e7d"
   },
   "source": [
    "##### **Formatting input for online prediction**\n",
    "\n",
    "This notebook uses [Torchserve's KServe based inference API](https://pytorch.org/serve/inference_api.html#kserve-inference-api) which is also [Vertex AI Predictions compatible format](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements#prediction). For online prediction requests, format the prediction input instances as JSON with base64 encoding as shown here:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"data\": {\n",
    "            \"b64\": \"<base64 encoded string>\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "646e9dfe8add"
   },
   "source": [
    "Define sample texts to test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "edc4658b2df1"
   },
   "outputs": [],
   "source": [
    "test_instances = [\n",
    "    b\"Jaw dropping visual affects and action! One of the best I have seen to date.\",\n",
    "    b\"Take away the CGI and the A-list cast and you end up with film with less punch.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52303da7c048"
   },
   "source": [
    "##### **Sending an online prediction request**\n",
    "\n",
    "Format input text string and call prediction endpoint with formatted input request and get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "a5808801dbcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Input text: \n",
      "\tJaw dropping visual affects and action! One of the best I have seen to date.\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"SmF3IGRyb3BwaW5nIHZpc3VhbCBhZmZlY3RzIGFuZCBhY3Rpb24hIE9uZSBvZiB0aGUgYmVzdCBJIGhhdmUgc2VlbiB0byBkYXRlLg==\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "Prediction response: \n",
      "\tPrediction(predictions=['Positive'], deployed_model_id='3203633835711397888', explanations=None)\n",
      "====================================================================================================\n",
      "Input text: \n",
      "\tTake away the CGI and the A-list cast and you end up with film with less punch.\n",
      "\n",
      "Formatted input: \n",
      "[\n",
      "    {\n",
      "        \"data\": {\n",
      "            \"b64\": \"VGFrZSBhd2F5IHRoZSBDR0kgYW5kIHRoZSBBLWxpc3QgY2FzdCBhbmQgeW91IGVuZCB1cCB3aXRoIGZpbG0gd2l0aCBsZXNzIHB1bmNoLg==\"\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "Prediction response: \n",
      "\tPrediction(predictions=['Negative'], deployed_model_id='3203633835711397888', explanations=None)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 100)\n",
    "for instance in test_instances:\n",
    "    print(f\"Input text: \\n\\t{instance.decode('utf-8')}\\n\")\n",
    "    b64_encoded = base64.b64encode(instance)\n",
    "    test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "    print(f\"Formatted input: \\n{json.dumps(test_instance, indent=4)}\\n\")\n",
    "    prediction = endpoint.predict(instances=test_instance)\n",
    "    print(f\"Prediction response: \\n\\t{prediction}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21664e69720a"
   },
   "source": [
    "##### ***[Optional]*** **Make prediction requests using gcloud CLI**\n",
    "You can also call the Vertex AI Endpoint to make predictions using [`gcloud beta ai endpoints predict`](https://cloud.google.com/sdk/gcloud/reference/beta/ai/endpoints/predict). \n",
    "\n",
    "The following cell shows how to make a prediction request to Vertex AI Endpoints using `gcloud` CLI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7bb062d3e7a8"
   },
   "outputs": [],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "950c04742e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REGION = us-central1\n",
      "ENDPOINT DISPLAY NAME = finetuned-bert-classifier-endpoint\n",
      "ENDPOINT_ID = 1220800954459226112\n",
      "INPUT TEXT = Take away the CGI and the A-list cast and you end up with film with less punch.\n",
      "PREDICTION RESPONSE = ['Negative']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Using endpoint [https://us-central1-prediction-aiplatform.googleapis.com/]\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $REGION $endpoint_display_name\n",
    "\n",
    "REGION=$1\n",
    "endpoint_display_name=$2\n",
    "\n",
    "# get endpoint id\n",
    "echo \"REGION = ${REGION}\"\n",
    "echo \"ENDPOINT DISPLAY NAME = ${endpoint_display_name}\"\n",
    "endpoint_id=$(gcloud beta ai endpoints list --region ${REGION} --filter \"display_name=${endpoint_display_name}\" --format \"value(ENDPOINT_ID)\")\n",
    "echo \"ENDPOINT_ID = ${endpoint_id}\"\n",
    "\n",
    "# call prediction endpoint\n",
    "input_text=\"Take away the CGI and the A-list cast and you end up with film with less punch.\"\n",
    "echo \"INPUT TEXT = ${input_text}\"\n",
    "\n",
    "prediction=$(\n",
    "echo \"\"\"\n",
    "{ \n",
    "   \"instances\": [\n",
    "     { \n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo ${input_text} | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "\"\"\" | gcloud beta ai endpoints predict ${endpoint_id} --region=$REGION --json-request -)\n",
    "\n",
    "echo \"PREDICTION RESPONSE = ${prediction}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e636b52e5913"
   },
   "source": [
    "## Cleaning up \n",
    "\n",
    "### Cleaning up training and deployment resources\n",
    "\n",
    "To clean up all Google Cloud resources used in this notebook, you can [delete the Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Training Jobs\n",
    "- Model\n",
    "- Endpoint\n",
    "- Cloud Storage Bucket\n",
    "- Container Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b89f5a348d1"
   },
   "source": [
    "Set flags for the resource type to be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26ee41d02418"
   },
   "outputs": [],
   "source": [
    "delete_custom_job = False\n",
    "delete_hp_tuning_job = False\n",
    "delete_endpoint = True\n",
    "delete_model = False\n",
    "delete_bucket = False\n",
    "delete_image = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fa19a6540bd2"
   },
   "source": [
    "Define clients for jobs, models and endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7f5f8dbd8cb6"
   },
   "outputs": [],
   "source": [
    "# API Endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)\n",
    "\n",
    "# Vertex AI location root path for your dataset, model and endpoint resources\n",
    "PARENT = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e41b7b75ef6"
   },
   "outputs": [],
   "source": [
    "# functions to create client\n",
    "def create_job_client():\n",
    "    client = aip.JobServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_model_client():\n",
    "    client = aip.ModelServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "def create_endpoint_client():\n",
    "    client = aip.EndpointServiceClient(client_options=client_options)\n",
    "    return client\n",
    "\n",
    "\n",
    "clients = {}\n",
    "clients[\"job\"] = create_job_client()\n",
    "clients[\"model\"] = create_model_client()\n",
    "clients[\"endpoint\"] = create_endpoint_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c41b12064678"
   },
   "source": [
    "Define functions to list the jobs, models and endpoints starting with `APP_NAME` defined earlier in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9a450d8eea2"
   },
   "outputs": [],
   "source": [
    "def list_custom_jobs():\n",
    "    client = clients[\"job\"]\n",
    "    jobs = []\n",
    "    response = client.list_custom_jobs(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def list_hp_tuning_jobs():\n",
    "    client = clients[\"job\"]\n",
    "    jobs = []\n",
    "    response = client.list_hyperparameter_tuning_jobs(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            jobs.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def list_models():\n",
    "    client = clients[\"model\"]\n",
    "    models = []\n",
    "    response = client.list_models(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            models.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return models\n",
    "\n",
    "\n",
    "def list_endpoints():\n",
    "    client = clients[\"endpoint\"]\n",
    "    endpoints = []\n",
    "    response = client.list_endpoints(parent=PARENT)\n",
    "    for row in response:\n",
    "        _row = MessageToDict(row._pb)\n",
    "        if _row[\"displayName\"].startswith(APP_NAME):\n",
    "            print(_row)\n",
    "            endpoints.append((_row[\"name\"], _row[\"displayName\"]))\n",
    "    return endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1281b5fe9222"
   },
   "source": [
    "#### **Deleting custom training jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "944b5ec4694c"
   },
   "outputs": [],
   "source": [
    "# Delete the custom training using the Vertex AI fully qualified identifier for the custom training\n",
    "try:\n",
    "    if delete_custom_job:\n",
    "        custom_jobs = list_custom_jobs()\n",
    "        for job_id, job_name in custom_jobs:\n",
    "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
    "            clients[\"job\"].delete_custom_job(name=job_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd76a52356ec"
   },
   "source": [
    "#### **Deleting hyperparameter tuning jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b65e57c07b43"
   },
   "outputs": [],
   "source": [
    "# Delete the hyperparameter tuning jobs using the Vertex AI fully qualified identifier for the hyperparameter tuning job\n",
    "try:\n",
    "    if delete_hp_tuning_job:\n",
    "        hp_tuning_jobs = list_hp_tuning_jobs()\n",
    "        for job_id, job_name in hp_tuning_jobs:\n",
    "            print(f\"Deleting job {job_id} [{job_name}]\")\n",
    "            clients[\"job\"].delete_hyperparameter_tuning_job(name=job_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc21008bb215"
   },
   "source": [
    "#### **Undeploy models and Delete endpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51684df7dc6b"
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint using the Vertex AI fully qualified identifier for the endpoint\n",
    "try:\n",
    "    if delete_endpoint:\n",
    "        endpoints = list_endpoints()\n",
    "        for endpoint_id, endpoint_name in endpoints:\n",
    "            endpoint = aiplatform.Endpoint(endpoint_id)\n",
    "            # undeploy models from the endpoint\n",
    "            print(f\"Undeploying all deployed models from the endpoint {endpoint_name}\")\n",
    "            endpoint.undeploy_all(sync=True)\n",
    "            # deleting endpoint\n",
    "            print(f\"Deleting endpoint {endpoint_id} [{endpoint_name}]\")\n",
    "            clients[\"endpoint\"].delete_endpoint(name=endpoint_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a6719f8f8a2"
   },
   "source": [
    "#### **Deleting models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "087a5322f387"
   },
   "outputs": [],
   "source": [
    "# Delete the model using the Vertex AI fully qualified identifier for the model\n",
    "try:\n",
    "    models = list_models()\n",
    "    for model_id, model_name in models:\n",
    "        print(f\"Deleting model {model_id} [{model_name}]\")\n",
    "        clients[\"model\"].delete_model(name=model_id)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "938a7cd172ab"
   },
   "source": [
    "#### **Delete contents from the staging bucket**\n",
    "\n",
    "---\n",
    "\n",
    "***NOTE: Everything in this Cloud Storage bucket will be DELETED. Please run it with caution.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a50bd867c02d"
   },
   "outputs": [],
   "source": [
    "if delete_bucket and \"BUCKET_NAME\" in globals():\n",
    "    print(f\"Deleting all contents from the bucket {BUCKET_NAME}\")\n",
    "\n",
    "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
    "    print(\n",
    "        f\"Size of the bucket {BUCKET_NAME} before deleting = {shell_output[0].split()[0]} bytes\"\n",
    "    )\n",
    "\n",
    "    # uncomment below line to delete contents of the bucket\n",
    "    # ! gsutil rm -r $BUCKET_NAME\n",
    "\n",
    "    shell_output = ! gsutil du -as $BUCKET_NAME\n",
    "    if float(shell_output[0].split()[0]) > 0:\n",
    "        print(\n",
    "            \"PLEASE UNCOMMENT LINE TO DELETE BUCKET. CONTENT FROM THE BUCKET NOT DELETED\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Size of the bucket {BUCKET_NAME} after deleting = {shell_output[0].split()[0]} bytes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afd19ee7efa7"
   },
   "source": [
    "#### **Delete images from Container Registry**\n",
    "\n",
    "Deletes all the container images created in this tutorial with prefix defined by variable `APP_NAME` from the registry. All associated tags are also deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d45ec639148b"
   },
   "outputs": [],
   "source": [
    "gcr_images = !gcloud container images list --repository=gcr.io/$PROJECT_ID --filter=\"name~\"$APP_NAME\n",
    "\n",
    "if delete_image:\n",
    "    for image in gcr_images:\n",
    "        if image != \"NAME\":  # skip header line\n",
    "            print(f\"Deleting image {image} including all tags\")\n",
    "            !gcloud container images delete $image --force-delete-tags --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3541e1629c02"
   },
   "source": [
    "### Cleaning up Notebook Environment\n",
    "\n",
    "After you are done experimenting, you can either [STOP](https://cloud.google.com/ai-platform/notebooks/docs/shut-down) or DELETE the AI Notebook instance to prevent any  charges. If you want to save your work, you can choose to stop the instance instead.\n",
    "\n",
    "```\n",
    "# Stopping Notebook instance\n",
    "gcloud notebooks instances stop example-instance --location=us-central1-a\n",
    "\n",
    "\n",
    "# Deleting Notebook instance\n",
    "gcloud notebooks instances delete example-instance --location=us-central1-a\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "n9qywopnIrJH",
    "545PP3o8IrJV",
    "7k8ge1L1IrJk"
   ],
   "name": "pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
